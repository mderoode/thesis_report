\label{section:resourceperspective} 
This chapter presents an overview of the state-of-the-art resource-perspective analysis in the context of process mining and aims to answer resource question RQ1. Moreover, this chapter also briefly reviews the state-of-the-art papers and contains explores various possibilities for further work. This state-of-the-art resource-perspective analysis aims to include previous research which meets the following selection criteria: the paper has as subject both \textit{process} or \textit{process mining} and \textit{resource}, whereas the latter can also be indicated as \textit{user}, \textit{organization}, \textit{human} or \textit{role}. The analysis, furthermore, used the following tooling to find relevant materials: \textit{WorldCat Discovery}, \textit{IEEE Xplore}, \textit{Springer Link} and \textit{Google Scholar}. Moreover, references in the found papers are also evaluated according to the selection criteria. 

As mentioned in the previous chapter, the resource perspective focuses on which individual resources perform which events. Furthermore, as also mentioned in previous chapter this thesis focuses on durable resources. Currently available research on the resource perspective also tends to focus on durable resources and can be categorized using the following groups: 

\begin{itemize}
\item General statistics of resources
\item Resource allocation rules and recommendation
\item Resource prioritization policies
\item Resource as queues
\item Conformance checking on resource perspective
\item Role Mining
\item Resource behaviour simulation 
\end{itemize} 

The following sections briefly summaries the available literature of these categories including a discussion of their results. However, simulating resource behaviour is out-of-scope for this thesis because this thesis focuses on analyzing behaviour observed in the past from event logs for which simulation is not a suitable technique. 

\section{General Statistics of Resources} \label{section:generalstatistics}
Several researchers created an approach to measure general statistics of resources. Pika et al. \cite{Pika2015,Pika2017} created a framework for analyzing and evaluating resource behaviour in the form as metrics such as productivity, skills, utilization, preferences, productivity and collaboration. All these measures, except collaboration, focus on an individual research and are defined for the case, activity and performance levels. The latter meaning that the metrics are defined on different granularity levels. Furthermore, the metrics are all visualized over time and changes are detected using a context-free outlier and 'change-point' detection algorithm. The metrics are implemented in an open-source proof-of-concept ProM plugin and is validated using a real dataset. 

Additionally, Huang et al. \cite{Huang2012} created a similar approach by measuring resource preference, availability, competence and cooperation. The metrics are essentially similar to \cite{Pika2015,Pika2017} but the implementation contains different visualization methods. Instead of visualizing the metrics over time, social networks are generated which can help identifying similar or related resources. Finally, \cite{Huang2012} creates a rather basic approach of recommending event scheduling based on the defined measures. This recommendation engine does, however, not take into account the current workload of resources, which seems an essential variable. The recommendation algorithm is implemented in a, rather limited, open-source ProM plugin.


\cite{Nakatumba2013} \todo{add Nakatumba}
The discussed papers contain valuable metrics, however, they do not elaborate on these metrics by, for example, correlating changes in these metrics with process outcome, quality or throughput time. Furthermore, the metrics and their changes over time are only measured, there is not yet invested any effort in finding the cause of sudden metric changes.  

\section{Resource Allocation Rules and Recommendation} \label{section:allocation}
This section contains the current research on Resource Allocation Rules (RAR), which can be described as rules or patterns which help to allocate events to the best available resource. Furthermore, this section contains various approaches to recommend the best resource for a given event. Huang et al. \cite{Huang2012} is already discussed in the previous section and contains a basic allocation recommendation algorithm based on the preference and competence of resources. 

Zhao et al. \cite{Zhao2015} created a more elaborate resource allocation model by relating resource allocation and process performance. The authors aim to minimize process throughput time in terms of resource preference, cost constraints and resource availability criteria. The latter meaning that the current workload of resources is taken into account, which is already a significant improvement over \cite{Huang2012}. However, the resource availability is also depended on the 'resource potential', which is a combination of the following variables: age, gender, family status and educational status. Defining the resource potential in this way is a questionable approach. The algorithm implementation is not public and the validation is based on an unspecified dataset.

Arias et al. \cite{arias2015framework} created a dynamic resource allocation framework which is based on resources capabilities, past performance, and resources workload. In practice, this means that the paper uses the following six dimensions: \textit{frequency} i.e. how many times a resource performed a similar activity, \textit{performance} i.e.the average execution time of the resource, \textit{quality} i.e. customer evaluation of past performance, \textit{cost} i.e. costs of execution the event, \textit{expertise} i.e. 'ability level' of the resource for a certain activity and finally, \textit{workload}, i.e. the current idle level of the resource. The six dimensions could be extended with other dimensions such as cooperation level as is already shown by \cite{Huang2012}. The work is implemented but not publicly available, and is validated using a non-public dataset. 

Liu et al. \cite{Liu2012} created an apriori-based algorithm for unsupervised Resource Allocation Rule mining. These rules are in the form of $activity \to resource$, meaning that for each activity a resource is recommended. Each 'rule' contains a support, confidence and lift metric, and the rule with the highest confidence (which meets a certain support and lift threshold) is chosen as the best recommendation. This method is rather naive as it does not take into account time perspective and the control-flow perspective, meaning that changes over time are not captured in this model and that activities which take place at different places in a process are also not taken into account. Furthermore, the resource workload and the data perspective are also not considered. Moreover, the algorithm is not publicly available but is validated on a public dataset. 

Huang et al.\cite{Huang2011} elaborated on this research by creating a more general unsupervised learning algorithm which takes also into account arbitrary data attributes. This is a significant improvement because other data attributes might contain valuable information and can potentially improve the resource allocation rule mining. The algorithms are discussed in great detail, however, the implementation is not public. The validation is limited to only a simple synthetic dataset. While the concepts of this research seem more promising as \cite{Liu2012}, performance issues might occur on large datasets because the algorithm seems computational expensive. Furthermore, resource workload, case attributes, data attributes of previous events and the time perspective are not taken into account because of the same performance issues. 

Finally, Huang et al. \cite{Huang2011a} created a more elaborate semi-supervised reinforcement-learning-based resource allocation recommendation engine. The algorithm is also discussed in great detail and a public ProM implementation is also available. The method is, however, only validated using a synthetic dataset and the method can only optimizer a single dimension (e.g. costs or throughput time). 

Most of the currently available research uses a rather basic machine learning approaches, while more elaborate machine learning techniques are already developed. These new techniques can help in three ways, namely: finding more advanced patterns, better filtering uninteresting rules and better performance. The latter is especially important because event logs can be enormous in size and can contain hundreds of data attributes. Simple naive data mining techniques cannot handle these quantities. Furthermore, the identified rules could be grouped and generalized such they become useful for end users \footnote{Note that the discussed approaches generate thousands of highly similar rules.}

\section{Resource Prioritization Policies} \label{section:prioritization}
Resource prioritization policies can be described as methods of how resources handle their own tasks on their worklist. Suriadi et al. \cite{Suriadi2015} investigated the resource prioritization policies by defining an approach of measuring which policies are followed. The authors focussed on FiFo, LiFo and prioritized FiFo. The latter being an extension of FiFo where a certain attribute is taken into account which denotes the priority level of an event. Their approach is to generate the worklists for all resources at all points in time and by simply walking through the log and assigning a penalty to each prioritization policy violation. The authors then visualize the penalty per resource, case or activity over time. Due to the nature of the penalty function, the algorithms do not find which prioritization policy is followed, but rather provide a relative scale on how much violations occur for each policy. It is not trivial to interpret this penalty value as it does not imply which prioritization policy is followed. Furthermore, there are more advanced prioritization policies which are not yet taken into account. Finally, many processes contain some kind of batch processing which interferes with the penalty score. The algorithms are implemented into an open-source ProM plugin and are validated using a synthetic and real dataset. 

There is no other research available in this field. However, the current work can easily be extended. For example, instead of just identifying which prioritization policies are followed, it might be interesting to measure the effects of such a prioritization policy on the event/process outcome, quality or performance. Furthermore, resources can be clustered based on their prioritization policies and changes in time can be detected and their cause and effect can be analysed. Sudden changes in prioritization policy might indicate changes in process definition or special periods such as holidays. 

\section{Resources as Queues} \label{section:queues}
Queueing theory is the mathematical study of waiting lines \cite{gross2008fundamentals, wolff1989stochastic}. The basic concept of queueing theory is that a durable resource needs to be shared among different users. The servers (resources) can only work simultaneously on a limited amount of tasks (often just a single task in the context of queueing theory), and thus a waiting line (or queue) of tasks appears. Queueing theory has a strong mathematical foundation and includes many prediction methods such as calculating the resource utilization or the average queue length. In the context of the resource perspective of process mining, it is possible to see resources as \textit{servers} and the worklist/backlog of allocated events as a queue. 

This is exactly what Senderovich et al. researched in \cite{Senderovich2014}. The authors created a method which is based on Queueing theory and using the available prediction methods of Queueing theory to predict delays in a process. The method is validated on a real dataset, however, the implementation is not publicly available. Furthermore, the method is limited to  homogeneous case types. The latter limitation is solved in a future work of the authors \cite{Senderovich2015b}, where the method can take into account different case types (such as customer types). This method outperforms the previous method. Both methods are described in great mathematical detail. However, just as the former method, there is also no implementation available for the latter method. 

Furthermore, Senderovich et al. the \cite{Senderovich2015a} also approached to join process mining techniques and queue mining techniques for performance analyzing by introducing Queue Enabling Colored Stochastic Petri Nets (QCSPN). This work is an initial step in closing the gap between these two techniques. Again no implementation is available, but the methods are validated on a real dataset. 

Because the absence of an open-source implementation, it is hard to compare the methods to other prediction approaches such as data mining algorithms. It would be interesting to compare different data mining algorithms against the queueing theory prediction on real data. Finally, the current work only focuses on predicting the case throughput time, but traditional queueing theory consists of much more predictions such as average workitems in queue and resource utilization rate. It would also be intersting to see how these predictions perform on real data.

\section{Conformance Checking on Resource Perspective}
Conformance checking traditionally focuses on the control-flow perspective and aims to find deviations from a policy in indivudal cases. Such a policy can be a predefined process structure, but also just a set of rules. Until recently, there was very little focus on the resource perspective as the only resource perspective specific conformance checking method was the 'four-eyed principle', where two events should not be executed by the same resource. 

De Leoni et al. \cite{DeLeoni2012} proposes a conformance checking approach which takes into account the resource perspective and can abstract from the data and control-flow perspectives. Traditional conformance checking algorithms first align the event log with the control flow, while this method concerns primarily about the resource perspective. Although this approach seems promising, it is only validated using a simple synthetic dataset. However, the implementation is available in an open-source ProM plugin. 

Mannhardt \cite{Mannhardt2018} created an extensive conformance checking framework which can balance the control-flow, data, resources, time perspectives. Furthermore, the author defined multi-perspective versions of traditional process mining metrics such as precision. The methods are validated using multiple real datasets in four case studies and the implementation is available as open-source ProM plugin. 

Because of the generalization of the previously discussed frameworks, most conformance checking problems can be solved by these frameworks. A possible further step could be to automatically identify the effects, or perhaps even causes, of the deviations. 

\section{Role Mining}\label{section:rolemining} \todo{add role clustering}
Role Mining emerged after the introduction of Role-based Access Control (RBAC) \cite{rbac2,rbac} and aims to group resource permissions in such a way that optimal roles can be constructed. RBAC is an access control approach which aims to manage resource permissions by defining resource roles. Each role contains a set of permissions on objects (e.g. files, processes, systems). Furthermore, users can be part of one or more roles. Before RBAC, organizations either used Discretionary Access Control (DAC) \cite{dac} or Mandatory Access Control (MAC) \cite{mac} which manages permissions on an individual resource basis. Because roles are more static over time than individual resources, the management of the security policy becomes less time-consuming \cite{rbac2,rbac}. In order to migrate from these old security policies to RBAC, roles should be defined. and thus \textit{role engineering} was created \cite{coyne1996role,roeckle2000role}. This is a set of techniques to define roles using a top-down approach, namely analyzing the organization and defining the roles based on domain knowledge. 

The top-down approach is time-consuming and rather error-prone, especially for large organizations with tens of thousands of employees and objects \cite{Kuhlmann2003}. Therefore, a bottom-up approach is created which analyzes all the permissions and tries to cluster the permissions in optimal roles without any domain knowledge using unsupervised machine learning algorithms. The input for the algorithm is a binary $User \; x \; Permission$ matrix, i.e. each user either has or does not have a certain permission. The output of the algorithm is two matrices, namely: a $User \; x\;  Role$ matrix and a $Role\;  x \; Permission$ matrix. The former matrix describes which roles the users have (N:M relation), and the latter matrix describes which permissions are each role has (also a N:M relation). The goal of the algorithm is to find clusters which are compatible with the old situation (i.e. no new permissions are allocated and no permissions are revoked) and to optimize the usability of roles (i.e. one could simply create a unique role for each user, but this is not against the principles of RBAC).

Kuhlmann et al. \cite{kuhlmann2003role} exactly followed this approach and used a clustering technique to identify the best roles. The techniques are evaluated on two non-public datasets using without quantitative metrics. Schlegelmilch et al. \cite{Schlegelmilch2005} followed another approach, namely by using subset enumeration on all permissions in order to find  all roles. The authors created two version of their algorithm, one which finds all roles but is rather inefficient, and a more efficient algorithm which has no guarantee that it founds all roles. The techniques are not evaluated in the paper, nevertheless, there is an open-source implementation available.

Zhang et al. \cite{zhang2007role} approached role mining as a graph optimization problem, where each user and permission are nodes and each edge represents an user owning a certain permission. The algorithm takes into account hierarchical roles and finds the optimal hierarchy based on the iterative merging of clustering. However, the algorithm is only evaluated on a small dataset and the implementation is not public ally available. 

These Role Mining are all based on the fact that all permissions are binary. Furthermore, the input of role mining is a matrix of $User \; x \; Permission$. This data is, however, not available in traditional process mining. The only related data which is available is the number of activities which resources perform over time. It is possible to convert this information to a binary $User \; x \; Permission$ table, but this would yield a lot of data loss (e.g. a resource which performs an activity 10,000 times is equal to a resource which performs the same activity a single time). It might be interesting to create a role mining algorithm which is tailored for a process mining scenario, in other words: it takes an event log as input instead of the $User \; x \; Permission$ matrix. 

\section{Conclusion}
The previous sections described the available state-of-the-art research on process mining and the resource perspective as is required for research question RQ1. For most categories, there is plenty of possible further work with the exception of conformance checking on resource perspective. In general, most work focus on identifying certain artifacts but do contain sufficient analysis to identify the (root)-cause of the artifact. Furthermore, the presentation of certain identified artifacts is suboptimal, therefore making it more challenging to use it in a business scenario instead of a research scenario. Finally, for role mining, there is no implementation which is tailored for a process mining setting.

These points can be used as starting point for defining resource-related research questions as is discussed in the next chapter. Each research question can be further decomposed in concrete problems which can be solved when the research question is answered. These problems can be identified and prioritized by performing structured interviews of domain experts.








